{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[x] Utilize o dataset Titanic\n",
    "\n",
    "[x] - Realize o pre-processamento dos dados: limpeza, tratamento de valores faltantes, conversão de dados categóricos vetorização, normalização.\n",
    "\n",
    "[] - Avalie um rede MLP utilizando o método de validação-cruzada com holdout. Varie o número de camadas ocultas (1,2,3) e de neurônios em cada camada oculta (16, 32,64). Use 10% do conjunto de treino para validação.\n",
    "\n",
    "[] - Use como função de ativação nas camadas ocultas a função ReLU e na camada de saída a softmax.\n",
    "\n",
    "[] - Para o treinamento utilize RMSprop como otimizador, entropia cruzada como função de loss (sparse_categorical_crossentropy). \n",
    "\n",
    "[] - Treine utilizando parada antecidapa: EarlyStopping como callback. Veja exemplo. \n",
    "\n",
    "[] - Monitore a acurácia durante o treino. Armazene as informações de treino usando history = model.fit(...). Imprima um gráfico que apresente a variação da acurácia de treino e de validação no decorrer das épocas. Imprima também um outro gráfico para a loss. Você encontra exemplos de como plotar essa informção aqui.\n",
    "\n",
    "[] - Avalie o melhor modelo com os dados de teste: model.evaluate(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"./titanic/train.csv\")\n",
    "test_data = pd.read_csv(\"./titanic/test.csv\")\n",
    "\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muitos valores da idade estão faltando, logo, tenho duas opções:\n",
    "    [] Remover a coluna.\n",
    "    [x] Colocar algum valor substituto, já que a idade na hora de salvar foi importante (Ex: crianças foram salvas primeiro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adicionar_idade(row):\n",
    "    if pd.isnull(row[\"Age\"]):\n",
    "        return newAge\n",
    "    else:\n",
    "        return row[\"Age\"]\n",
    "\n",
    "newAge = train_data[\"Age\"].median() # Pego a mediana das idades.\n",
    "train_data['Age'] = train_data.apply(adicionar_idade, axis = 1)\n",
    "\n",
    "newAge = test_data[\"Age\"].median() # Pego a mediana das idades (Não a média).\n",
    "test_data['Age'] = test_data.apply(adicionar_idade, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Será que a cabine que a pessoa estava tem relação com sua sobrevivência?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_survival = train_data.groupby(\"Cabin\")[\"Survived\"].mean()\n",
    "print(cabin_survival)\n",
    "# Percebemos que dependendo da Cabine a pessoa tem mais chances de sobreviver, algumas 100%, outras 0%..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinNames = set(train_data['Cabin'])\n",
    "mapping = [i for i in range(len(cabinNames))]\n",
    "train_data['Cabin'].replace(cabinNames, mapping, inplace=True) # Mapeando para inteiro\n",
    "\n",
    "cabinNames = set(test_data['Cabin'])\n",
    "mapping = [i for i in range(len(cabinNames))]\n",
    "test_data['Cabin'].replace(cabinNames, mapping, inplace=True) # Mapeando para inteiro\n",
    "\n",
    "train_data['Cabin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lidando com o último atributo faltante (Fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_median = test_data[\"Fare\"].median()\n",
    "test_data[\"Fare\"].fillna(fare_median, inplace=True) # Preenchendo valores nulos com a mediana\n",
    "\n",
    "test_data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Será que o porto de embarque tem relação com sua sobrevivência?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_survival = train_data.groupby(\"Embarked\")[\"Survived\"].mean()\n",
    "print(embarked_survival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarkedNames = set(train_data['Embarked'])\n",
    "mapping = [i for i in range(len(embarkedNames))]\n",
    "train_data['Embarked'].replace(embarkedNames, mapping, inplace=True) # Mapeando para inteiro\n",
    "\n",
    "embarkedNames = set(test_data['Embarked'])\n",
    "mapping = [i for i in range(len(embarkedNames))]\n",
    "test_data['Embarked'].replace(embarkedNames, mapping, inplace=True) # Mapeando para inteiro\n",
    "\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo atributo categórico Sex para numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Sex\"].replace([\"male\", \"female\"], [0, 1], inplace=True)\n",
    "test_data[\"Sex\"].replace([\"male\", \"female\"], [0, 1], inplace=True)\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apagando colunas inúteis para a análise (Colunas como nome e Ticket, além de Survived já que é usada no y_train)\n",
    "X_train = train_data.drop(['Survived', 'Name', 'Ticket','PassengerId'], axis=1)\n",
    "# Pegando o y\n",
    "y_train = train_data['Survived']\n",
    "\n",
    "# Apagando colunas inúteis para a análise nos dados de teste\n",
    "X_test = test_data.drop(['Name', 'Ticket','PassengerId'], axis=1)\n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizando dados de treino, validação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "normalized = StandardScaler()\n",
    "normalized.fit(X_train)\n",
    "\n",
    "X_train_std = normalized.transform(X_train)\n",
    "X_test_std = normalized.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20)\n",
    "\n",
    "maxAcc = float(\"-inf\")\n",
    "numLayers = [1, 2, 3]\n",
    "numNeurons = [16, 32, 64]\n",
    "\n",
    "bestNumLayers = numLayers[0]\n",
    "bestNumNeuron = numNeurons[0]\n",
    "\n",
    "\n",
    "for nLayer in numLayers:\n",
    "    for nNeuron in numNeurons:\n",
    "        inter = 0\n",
    "        sqLayers = [Dense(nNeuron, activation=\"relu\")] * (nLayer - 1) + [\n",
    "            Dense(2, activation=\"softmax\")\n",
    "        ]\n",
    "        model = Sequential(sqLayers)\n",
    "        model.compile(\n",
    "            optimizer=\"rmsprop\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\", \"top_k_categorical_accuracy\"],\n",
    "        )\n",
    "        history = model.fit(\n",
    "            x=X_train_std,\n",
    "            y=y_train,\n",
    "            epochs=100,\n",
    "            callbacks=[callback],\n",
    "            validation_split=0.1,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        if np.mean(history.history[\"val_accuracy\"]) > maxAcc:\n",
    "            bestNLayers = nLayer\n",
    "            bestNNeuron = nNeuron\n",
    "\n",
    "        print(f\"Quantidade de camadas ocultas: {nLayer}\")\n",
    "        print(f\"Quantidade de neurônios nesta camada: {nNeuron}\\n\\n\")\n",
    "\n",
    "        #  Variação da acurácia de treino e de validação\n",
    "        plt.plot(history.history[\"accuracy\"])\n",
    "        plt.plot(history.history[\"val_accuracy\"])\n",
    "        plt.title(\"Acurácia do modelo\")\n",
    "        plt.ylabel(\"Acurácia\")\n",
    "        plt.xlabel(\"Época\")\n",
    "        plt.legend([\"Treino\", \"Validação\"], loc=\"bottom right\")\n",
    "        plt.show()\n",
    "\n",
    "        # Variação do loss\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        plt.plot(history.history[\"val_loss\"])\n",
    "        plt.title(\"Perdas do modelo\")\n",
    "        plt.ylabel(\"Perca\")\n",
    "        plt.xlabel(\"Época\")\n",
    "        plt.legend([\"Treino\", \"Validação\"], loc=\"bottom right\")\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando a predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apagando colunas inúteis para a análise (Colunas como nome e Ticket, além de Survived já que é usada no y_test)\n",
    "# y_pred = dt.predict(X_test)\n",
    "# to_submit = pd.concat([test_data.PassengerId, pd.DataFrame({\"Survived\": y_pred})], axis=1)\n",
    "\n",
    "# to_submit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportando para fazer a predição no site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_submit.to_csv(\"output.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
